{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741665d9-11c0-4629-b902-f34c7f6ad1b0",
   "metadata": {},
   "source": [
    "# Projet SAM -> Classifieur de genre musical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832ee15-bc5f-4e2b-9b7b-ca2322eba7b9",
   "metadata": {},
   "source": [
    "- Reference Paper for CNN: https://www.ijert.org/image-classification-using-hog-and-lbp-feature-descriptors-with-svm-and-cnn\n",
    "- Reference Tutorial for CNN:\n",
    "https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n",
    "- Reference Tutorial for Transfer Learning: https://keras.io/guides/transfer_learning/\n",
    "- Keras pre-trained ResNet: https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c5c84-9479-4195-9cb7-4f001180d971",
   "metadata": {},
   "source": [
    "## Classifieur des Images \n",
    "#### Partie 1: Regression Logistic & CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca5c7d-0514-4bfa-b9e2-d18568698f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import imutils\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8818137-5d25-4f1a-95b3-423bd31d8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_list(msdi_path):\n",
    "    df = pd.read_csv(msdi_path+'labels.csv', header=None)\n",
    "    return list(df.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868abc7-b0b3-4067-a582-3f1e43f499e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"C:/AMU/SAM/Projet/msdi/\" #C:\\AMU\\SAM\\Projet\\msdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee563e49-a2e2-483e-ad7b-4d7da63f8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = pd.read_csv(main_path+\"msdi_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25922916-b31b-431b-bab4-d41807c08e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = entries[entries['set'] == \"test\"]\n",
    "train = entries[entries['set'] == \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134b4ba-7152-4ced-9166-00d75a740cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = get_label_list(main_path)\n",
    "labels = {label_list[i]:i for i in range (0,len(label_list))} # Dictionary Etiquetes Possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e8a7-fa11-4bc8-9ab6-9da66f96cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca80f0f-309e-4cef-969f-9f4efc007d77",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4d4d9-ebc3-40b2-8632-5fcff7a64e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing\n",
    "\n",
    "\"\"\"\n",
    "cv2.imshow(\"teste\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dee14-ffee-4601-a29c-995f1840d0ee",
   "metadata": {},
   "source": [
    "### Read the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246f99df-a306-4146-b883-b8b715169568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    #img_path = msdi_path+entry['img']\n",
    "    if os.path.exists(img_path):\n",
    "        return cv2.imread(img_path)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0dac55-955b-44ed-b56e-b78a02da7e6f",
   "metadata": {},
   "source": [
    "### Aplly Hog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8bae66-a619-44e0-9db7-b9cd37f693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_through(image_set, image_labels):\n",
    "    file_list = []\n",
    "    for i in range(len(image_set)):\n",
    "        if i%1000 == 0:\n",
    "            print(\"check point:\",i)\n",
    "            \n",
    "        img = load_img(main_path+image_set[i])\n",
    "\n",
    "        if img.any():\n",
    "            \n",
    "            #resized_img = imutils.resize(img, width=100, height=100)\n",
    "            fd, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1),\n",
    "                            visualize=True,\n",
    "                            feature_vector=True, channel_axis = 2) # channel_axis = 2 pour les images colores\n",
    "            #print(i, labels[image_labels[i]])\n",
    "            \n",
    "            fd = fd.tolist()\n",
    "            fd.append(labels[image_labels[i]]) # Make a vector (hogged_image = {h1,h1...hn}+ {image label (0-14)})\n",
    "            file_list.append(fd)\n",
    "    return pd.DataFrame.from_records(file_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f06baf-53dc-4c02-a1a8-f30845a34750",
   "metadata": {},
   "source": [
    "### Save Images as Hog Vectors + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad6860fc-1aa8-472b-931b-5434b2c1d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df, file_name):\n",
    "    df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a0c4fa-9c33-4c86-ac2b-f69ccc91d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = list(train['img'])\n",
    "train_labels = list(train['genre'])\n",
    "\n",
    "test_images = list(test['img'])\n",
    "test_labels = list(test['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ae0fd7-19bf-4625-8b4e-b555388da39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point: 0\n",
      "check point: 1000\n",
      "check point: 2000\n",
      "check point: 3000\n",
      "check point: 4000\n",
      "check point: 5000\n",
      "check point: 6000\n",
      "check point: 7000\n",
      "check point: 8000\n",
      "check point: 9000\n",
      "check point: 10000\n",
      "check point: 11000\n",
      "check point: 12000\n",
      "check point: 13000\n",
      "check point: 14000\n",
      "check point: 15000\n",
      "check point: 16000\n",
      "check point: 17000\n",
      "check point: 18000\n",
      "check point: 19000\n",
      "check point: 20000\n",
      "check point: 21000\n"
     ]
    }
   ],
   "source": [
    "train_csv = hog_through(train_images, train_labels)\n",
    "save_csv(train_csv,\"train_hog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fba4fb8-b32a-44b1-bec5-018101cdb340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point: 0\n",
      "check point: 1000\n",
      "check point: 2000\n",
      "check point: 3000\n",
      "check point: 4000\n"
     ]
    }
   ],
   "source": [
    "test_csv = hog_through(test_images, test_labels)\n",
    "save_csv(test_csv,\"test_hog.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dcbd6-8e3a-4828-9b06-df1db9c214d4",
   "metadata": {},
   "source": [
    "### Reading the Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a8ed8-440f-42d0-ae32-138fd3d60605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cc5fbf3-6635-4837-be18-e49b1dc7a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_label_split(df):\n",
    "    x = df.iloc[:,0:(len(df.columns)-1)]\n",
    "    y = df.iloc[:,(len(df.columns)-1):(len(df.columns))]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99459de6-d46c-415c-9163-c1c633acb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (no need to run this if you just run the data processing part, use train_CSV and test_CSV)\n",
    "train_csv = pd.read_csv(\"C:/AMU/SAM/Projet/train_hog.csv\")\n",
    "test_csv = pd.read_csv(\"C:/AMU/SAM/Projet/test_hog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a59972bd-c9a6-4710-af31-50c39629b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data_label_split(train_csv)\n",
    "x_test, y_test = data_label_split(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eafe0ba-de02-4acc-b3ef-353944cb4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this later \n",
    "\"\"\"\n",
    "x_train = x_train.drop(x_train.columns[[0]], axis=1)\n",
    "x_test = x_test.drop(x_test.columns[[0]], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9e17f-509e-4ce9-a0e4-08c64ca94e53",
   "metadata": {},
   "source": [
    "### Regression Logistique + HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81e8850c-0c89-44da-bbae-f8f8f9e865ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fafa529-83b9-4c7a-8e0d-4d794b15fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing.normalize(x_train)\n",
    "x_test = preprocessing.normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbf81905-71d8-4abb-836d-cf5decec72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial',max_iter=300) #'lbfgs' 'saga'\n",
    "logi.fit(x_train, y_train.to_numpy().ravel())\n",
    "logi_predic = logi.predict(x_test)\n",
    "logi_accu = accuracy_score(y_test.to_numpy().ravel(), logi_predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9b80d1e-a7d2-4591-a269-2c1f8bf30509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20692622069262206"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f2805-a458-4b37-9dde-a9f884982902",
   "metadata": {},
   "source": [
    "### CNN: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939e66c-8894-484c-be81-9099408230c4",
   "metadata": {},
   "source": [
    "### Load / Read Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5428945f-833d-4df7-b247-aee848f1a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b2565c-7b5e-4b2d-849e-e86c313a84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"C:/AMU/SAM/Projet/msdi/\" #C:\\AMU\\SAM\\Projet\\msdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d0f793-e029-437c-84a4-eee8c0d32d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(entry, msdi_path):\n",
    "    return cv2.imread(msdi_path+ entry['img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b09c860-b3db-42b5-952e-e3fd0b8cd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_through(entries, main_path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(entries)):\n",
    "        \n",
    "        entry = entries.iloc[i,:]\n",
    "        img = load_img(entry,main_path)\n",
    "        \n",
    "        y = labels[entry[\"genre\"]]\n",
    "        \n",
    "        X.append(img)\n",
    "        Y.append(y)\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2242c87d-8160-4a6c-ad36-39884e8cbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: RUN CELL 2 FOR THIS\n",
    "label_list = get_label_list(main_path)\n",
    "labels = {label_list[i]:i for i in range (0,len(label_list))} # Dictionary Etiquetes Possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c9c7ba-37b4-4a06-ab60-3ac1bb1c15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = pd.read_csv(main_path+\"msdi_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0be4a6-dc77-4aa6-aed5-23977ec8ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = entries[entries['set'] == \"train\"]\n",
    "validation = entries[entries['set'] == \"val\"]\n",
    "test = entries[entries['set'] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8239d7ad-388e-4c55-97dc-2b831cc6cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = read_through(train,main_path)\n",
    "x_test, y_test = read_through(test,main_path)\n",
    "x_validation, y_validation = read_through(validation,main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63ed06-e2b8-40d9-8ada-23be7fb29a31",
   "metadata": {},
   "source": [
    "#### Convert labels to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989a83b5-cf86-4410-a87f-97c103cb68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#one-hot encoding\n",
    "y_train = to_categorical(y_train, 15)\n",
    "y_validation = to_categorical(y_validation,15)\n",
    "y_test = to_categorical(y_test,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83181f4e-47dc-4eb6-be5e-53fb54ec8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train, dtype=np.float32)\n",
    "x_validation = np.asarray(x_validation, dtype=np.float32)\n",
    "x_test = np.asarray(x_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6dee0-9bf4-4b20-8410-d8366e02b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "type( train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c402bc23-84b0-49d1-ac5c-10cda1bacd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21383, 200, 200, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de6efd1-0b70-4018-a557-0f7a0bdb8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train = train[0:5000]\n",
    "y_train = y_train[0:5000]\n",
    "x_validation = validation[0:2000]\n",
    "y_validation = y_validation[0:2000]\n",
    "x_test = test[0:1000]\n",
    "y_test = y_test[0:1000]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e27836f8-33bb-4ace-b03f-61ed2158bf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673fc33-2999-498a-b198-0acfd46eefb6",
   "metadata": {},
   "source": [
    "#### Save NP arrays as npz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e39d443-b41e-4349-b492-6902d2dbcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(main_path+\"msdi_imgs_as_np\",  x_train= x_train, y_train =y_train ,\n",
    "          x_validation= x_validation,y_validation =y_validation ,\n",
    "          x_test= x_test,y_test =y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f89dc-8f35-4ee6-add3-cc5600d50563",
   "metadata": {},
   "source": [
    "#### Read NP arrays\n",
    "(run from here if npz file existent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682dc62b-61e4-47af-a6e3-6342f4c23ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(main_path+\"msdi_imgs_as_np_small.npz\") # _small train = 5000, val = 2000, test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b4322f-040e-4d30-a943-b8d16d169fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_train', 'y_train', 'x_validation', 'y_validation', 'x_test', 'y_test']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50e435-83ff-413a-a57c-6b91f3f03542",
   "metadata": {},
   "source": [
    "##### Don't run this, its just for reference\n",
    "- x_train = npzfile['x_train']\n",
    "- x_validation = npzfile['x_validation']\n",
    "- x_test = npzfile['x_test']\n",
    "\n",
    "- y_train = npzfile['y_train']\n",
    "- y_validation = npzfile['y_validation']\n",
    "- y_test = npzfile['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9caa42-df2a-4f71-b9ad-03bb1d751c32",
   "metadata": {},
   "source": [
    "#### Todo:\n",
    "- Run npzation for test and validation\n",
    "- Save the np arrays so i dont have to load this everytime for fs\n",
    "- Run the CNN training again with validation step\n",
    "- Cry over the bad accuracy results\n",
    "- Do the fine tuning to see if the results get any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b51b8b-85d8-4dd5-8c42-5c931d88d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 200, 200, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile['x_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccde096-cc5f-44a1-8935-eb48ae09db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e5e62-2d5a-4015-b358-53990015a058",
   "metadata": {},
   "source": [
    "### ResNet 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d43263-27b2-43d4-a65b-ee74dc758bc8",
   "metadata": {},
   "source": [
    "- utiliser un réseau ResNet déjà entrainé a\n",
    "- ajouter une couche de sortie (sigmoïde)\n",
    "- Exemple d’entrainement : mini-batch de 50 exemples, 90 epochs, learning rate 0.0001, optimiseur Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58738f92-1bba-4690-a790-2b8fac9e3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480d1f0-d099-429a-a352-f886b79f2917",
   "metadata": {},
   "source": [
    "### Initializing ResNet with imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "481176a5-2401-485e-a539-d4ba287a46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet',input_shape=(200, 200, 3),include_top=False)\n",
    "# Freeze layers\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e6ae56-2132-433c-951e-a1d2d8325a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to top\n",
    "inputs = keras.Input(shape=(200, 200, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier for 15 classes/ labels (Categorical classification)\n",
    "outputs = keras.layers.Dense(15,activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8d0f6-fa9a-4e70-93ed-9a254197ed4b",
   "metadata": {},
   "source": [
    "#### Compile model and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e16f5e6-ff0f-4a5c-a8cd-f9011debbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 515s 3s/step - loss: 2.0692 - categorical_accuracy: 0.4052 - val_loss: 2.1868 - val_categorical_accuracy: 0.3400\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 503s 3s/step - loss: 1.6326 - categorical_accuracy: 0.4886 - val_loss: 2.1881 - val_categorical_accuracy: 0.3630\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 489s 3s/step - loss: 1.4347 - categorical_accuracy: 0.5414 - val_loss: 2.1899 - val_categorical_accuracy: 0.3750\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 487s 3s/step - loss: 1.2965 - categorical_accuracy: 0.5820 - val_loss: 2.3483 - val_categorical_accuracy: 0.3595\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 486s 3s/step - loss: 1.1667 - categorical_accuracy: 0.6220 - val_loss: 2.2003 - val_categorical_accuracy: 0.3445\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 471s 3s/step - loss: 1.0754 - categorical_accuracy: 0.6516 - val_loss: 2.2971 - val_categorical_accuracy: 0.3550\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 478s 3s/step - loss: 0.9988 - categorical_accuracy: 0.6738 - val_loss: 2.3467 - val_categorical_accuracy: 0.3480\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 490s 3s/step - loss: 0.9342 - categorical_accuracy: 0.6934 - val_loss: 2.3799 - val_categorical_accuracy: 0.3350\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 485s 3s/step - loss: 0.8756 - categorical_accuracy: 0.7180 - val_loss: 2.4487 - val_categorical_accuracy: 0.3425\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 488s 3s/step - loss: 0.8281 - categorical_accuracy: 0.7318 - val_loss: 2.5145 - val_categorical_accuracy: 0.3315\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 499s 3s/step - loss: 0.7744 - categorical_accuracy: 0.7554 - val_loss: 2.5712 - val_categorical_accuracy: 0.3450\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 490s 3s/step - loss: 0.7315 - categorical_accuracy: 0.7730 - val_loss: 2.5700 - val_categorical_accuracy: 0.3415\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 498s 3s/step - loss: 0.6760 - categorical_accuracy: 0.7936 - val_loss: 2.5783 - val_categorical_accuracy: 0.3165\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 495s 3s/step - loss: 0.6536 - categorical_accuracy: 0.7952 - val_loss: 2.5769 - val_categorical_accuracy: 0.3250\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 487s 3s/step - loss: 0.6263 - categorical_accuracy: 0.8116 - val_loss: 2.6403 - val_categorical_accuracy: 0.3285\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 485s 3s/step - loss: 0.5871 - categorical_accuracy: 0.8294 - val_loss: 2.6798 - val_categorical_accuracy: 0.3250\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 475s 3s/step - loss: 0.5726 - categorical_accuracy: 0.8314 - val_loss: 2.6958 - val_categorical_accuracy: 0.3265\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 474s 3s/step - loss: 0.5356 - categorical_accuracy: 0.8440 - val_loss: 2.8186 - val_categorical_accuracy: 0.3345\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 479s 3s/step - loss: 0.5166 - categorical_accuracy: 0.8502 - val_loss: 2.8051 - val_categorical_accuracy: 0.3405\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 487s 3s/step - loss: 0.4887 - categorical_accuracy: 0.8642 - val_loss: 2.8969 - val_categorical_accuracy: 0.3295\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=keras.losses.CategoricalCrossentropy(from_logits=False),metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "history_cnn = model.fit( x_train,y_train,validation_data=(x_validation,y_validation), epochs=20, verbose = 1) #, validation_data=(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c8b0baa-c61d-4197-8783-1645c994237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/resnet_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43cea3f-36ad-46f1-8700-ce8b64fa2624",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f4ce5d5-1c57-43b9-b2d0-a42ad64aeb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7127900019288063"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(history_cnn.history[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1e86a88-c055-4eaa-b9a2-a0666938bcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8641999959945679"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_cnn.history[\"categorical_accuracy\"][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9e7daed-cfe1-4900-a54f-82102a4b73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 72s 2s/step - loss: 2.4000 - categorical_accuracy: 0.4370\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x=x_test, y=y_test, batch_size=None, verbose=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f520d326-4d5b-46ab-8110-20aa4c31aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc: [2.399991512298584, 0.43700000643730164]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87c59fab-aea4-4cb1-96b3-34da4a24bbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43700000643730164"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acd0a36d-0382-48b2-bb72-3557adbcd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_accu = results[1]\n",
    "reg_accu = 0.20692622069262206"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80cf15-dfa9-4de8-a4da-e9c7549e7040",
   "metadata": {},
   "source": [
    "#### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba12376-ec3f-4611-8c91-e76ed10754c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2582f5eb8b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recompile Model to run this\n",
    "model.load_weights('./checkpoints/resnet_checkpoint') # Transfer Learning trained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3208d07a-8702-4fba-8ea7-3918c16ae372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 1606s 10s/step - loss: 244950400.0000 - categorical_accuracy: 0.2212 - val_loss: 7.3246 - val_categorical_accuracy: 0.3215\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 1541s 10s/step - loss: 5.3458 - categorical_accuracy: 0.3934 - val_loss: 2.3711 - val_categorical_accuracy: 0.3540\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 1481s 9s/step - loss: 2.0732 - categorical_accuracy: 0.4466 - val_loss: 2.2387 - val_categorical_accuracy: 0.3540\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 1497s 10s/step - loss: 2.0398 - categorical_accuracy: 0.4470 - val_loss: 2.2653 - val_categorical_accuracy: 0.3540\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 1501s 10s/step - loss: 2.0455 - categorical_accuracy: 0.4470 - val_loss: 2.2364 - val_categorical_accuracy: 0.3540\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 1438s 9s/step - loss: 2.0324 - categorical_accuracy: 0.4470 - val_loss: 2.2342 - val_categorical_accuracy: 0.3540\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 1447s 9s/step - loss: 2.0370 - categorical_accuracy: 0.4470 - val_loss: 2.2569 - val_categorical_accuracy: 0.3540\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 1450s 9s/step - loss: 2.0233 - categorical_accuracy: 0.4470 - val_loss: 2.2176 - val_categorical_accuracy: 0.3540\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 1457s 9s/step - loss: 2.0172 - categorical_accuracy: 0.4470 - val_loss: 2.2125 - val_categorical_accuracy: 0.3540\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 5020s 32s/step - loss: 2.0086 - categorical_accuracy: 0.4470 - val_loss: 2.2304 - val_categorical_accuracy: 0.3540\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 1443s 9s/step - loss: 2.0036 - categorical_accuracy: 0.4470 - val_loss: 2.2602 - val_categorical_accuracy: 0.3540\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 1443s 9s/step - loss: 2.0180 - categorical_accuracy: 0.4470 - val_loss: 2.2246 - val_categorical_accuracy: 0.3540\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 1445s 9s/step - loss: 2.0053 - categorical_accuracy: 0.4470 - val_loss: 2.1982 - val_categorical_accuracy: 0.3405\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 1440s 9s/step - loss: 1.9966 - categorical_accuracy: 0.4470 - val_loss: 2.1995 - val_categorical_accuracy: 0.3530\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 1424s 9s/step - loss: 1.9951 - categorical_accuracy: 0.4466 - val_loss: 2.2290 - val_categorical_accuracy: 0.3540\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 8327s 53s/step - loss: 1.9982 - categorical_accuracy: 0.4464 - val_loss: 2.2295 - val_categorical_accuracy: 0.3540\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 1412s 9s/step - loss: 1.9834 - categorical_accuracy: 0.4476 - val_loss: 2.1974 - val_categorical_accuracy: 0.3540\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 5473s 35s/step - loss: 1.9850 - categorical_accuracy: 0.4464 - val_loss: 2.2450 - val_categorical_accuracy: 0.3520\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 1428s 9s/step - loss: 1.9909 - categorical_accuracy: 0.4468 - val_loss: 2.2149 - val_categorical_accuracy: 0.3540\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 1427s 9s/step - loss: 1.9820 - categorical_accuracy: 0.4460 - val_loss: 2.2557 - val_categorical_accuracy: 0.3540\n"
     ]
    }
   ],
   "source": [
    "# TODO: Not doing fine-tuning rn due to lack of time (come back here after finishing audio part + bandits (AR))\n",
    "\n",
    "# Unfreeze base , set 1e-5 as learning rate (avoid overfit)\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=keras.losses.CategoricalCrossentropy(from_logits=False),metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "history_cnn_ft = model.fit(npzfile['x_train'],npzfile['y_train'],validation_data=(npzfile['x_validation'],npzfile['y_validation']), epochs=20, verbose = 1) #, validation_data=(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e843b92c-73ba-4a38-811e-617a280450d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/resnet_ft_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e4286c4-785a-40c9-879f-64b04b957824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44600000977516174"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_cnn_ft.history['categorical_accuracy'][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04724a60-fa85-4fec-ba81-bd5a704f7b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4328999973833561"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(history_cnn_ft.history[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302f4984-0f88-4bcc-973a-faca43ab8874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 70s 2s/step - loss: 1.8710 - categorical_accuracy: 0.5020\n",
      "test loss, test acc: [1.871049165725708, 0.5019999742507935]\n"
     ]
    }
   ],
   "source": [
    "results_ft = model.evaluate(x=npzfile['x_test'], y=npzfile['y_test'], batch_size=None, verbose=1, sample_weight=None) # npzfile['x_test']\n",
    "print(\"test loss, test acc:\", results_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e6f6a-1270-4b67-ba2c-630f2559a3ad",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13966858-b0a0-49b9-ab48-cd86e75f2fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e66aabc6-ccb3-45af-991b-7e95fd805fb1",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "| Algorithm | max_iter | Accuracy |\n",
    "| --- | --- | --- |\n",
    "| lbfgs | 300 | 0.2069 |\n",
    "| lbfgs | 500 | 0.1632 |\n",
    "| lbfgs | 1000 | 0.1503 |\n",
    "| lbfgs | 3000 | 0.1525 |\n",
    "| saga | 300 | 0.1595 |\n",
    "| saga | 1000 | 0.1555 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eff3dd-5c59-4199-994d-e2bea299b9f8",
   "metadata": {},
   "source": [
    "#### CNN RestNet50 Train = 5000, Val = 2000, Test = 1000, model pre-trained with imagenet\n",
    "\n",
    "| Step | Learning Rate | Accuracy | Average Accuracy\n",
    "| --- | --- | --- | ---|\n",
    "| Transfer Learning: Trainig |0.001 | 0.8641 |0.712|\n",
    "| Transfer Learning: Test | - | 0.4370 | -| -\n",
    "| Fine-Tuning: Training | 1e-5 |0.446| 0.4329|\n",
    "| Fine-Tuning: Test | - | 0.502 | -| \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb2602c6-e615-4062-8e9c-15b0ad4cfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1db41340-26a7-4c60-9e59-426d8c71b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1 artists>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure()\n",
    "subplot = figure.add_subplot()\n",
    "\n",
    "labels = [\"Logistic Regression\", \"CNN\"]\n",
    "x = np.arange(len(labels))\n",
    "x = [1,4]\n",
    "subplot.set_xticks(x, labels)\n",
    "subplot.legend()\n",
    "subplot.set_title(\"Accuracy LogReg x CNN\")\n",
    "\n",
    "\n",
    "plt.bar(1, reg_accu, 1)\n",
    "\n",
    "plt.bar(4, cnn_accu, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee41b3-75a3-4220-9b7a-ec772fab4a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
